<!DOCTYPE html>
<html>
  <head>
    <title>Memoization Performance</title><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />
    <link href="assets/css/style.css" rel="stylesheet">
    <script src="assets/js/script.js" type="text/javascript"></script><script>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']]}};</script><script src="assets/vendor/mathjax/tex-svg.js" onerror="var s=document.createElement('script');s.src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js';document.head.appendChild(s);"></script>
    <link href="assets/vendor/prism/prism-tomorrow.min.css" rel="stylesheet">
    <script src="assets/vendor/prism/prism.min.js"></script>
    <script src="assets/vendor/prism/components/prism-python.min.js"></script>
    <script src="assets/vendor/prism/components/prism-javascript.min.js"></script><script src="assets/vendor/plotly/plotly.min.js" onerror="var s=document.createElement('script');s.src='https://cdn.plot.ly/plotly-2.32.0.min.js';document.head.appendChild(s);"></script><style>:root{--sidebar-width:240px;--content-padding-x:20px;}</style><style>.content-inner { max-width: 900px !important; }</style>
  </head>
  <body>
    <div id="sidebar">
      <a class="sidebar-title" href="index.html">Memoization Performance</a>
      <a class="nav-link" href="pages/memoization-demo.html">Memoization Demo</a>
      <div id="sidebar-footer"><a href="../index.html">← Back to Directory</a></div>
    </div>
    <div id="content">
      <div class="content-inner">
        <div>
          <div style="max-width: 900px; margin: 0 auto; width: 100%; padding-top: 80px; padding-bottom: 80px; --content-width: 900px;">
            <h1>Graph Memoization Performance</h1>
            <div>
              <p>When exploring parameter variants, pipelines automatically cache (memoize) intermediate results.
    This means shared computation stages only execute once, dramatically improving performance.
    
    This demo uses simple operations with artificial delays to clearly demonstrate the concept.</p>
            </div>
            <h2>Understanding Memoization</h2>
            <div>
              <p>Consider a graph that:
    1. Loads data (2 seconds - expensive)
    2. Preprocesses data (1.5 seconds - expensive)
    3. Applies different scaling factors (fast, varies per combination)
    
    If we explore 3 scaling factors:
    
    **Without memoization:**
    - Load data: 2s × 3 = 6 seconds
    - Preprocess: 1.5s × 3 = 4.5 seconds
    - Scale: 3 fast operations
    - **Total: ~10.5 seconds**
    
    **With memoization:**
    - Load data: 2s × 1 = 2 seconds (cached!)
    - Preprocess: 1.5s × 1 = 1.5 seconds (cached!)
    - Scale: 3 fast operations
    - **Total: ~3.5 seconds**
    
    **Expected speedup: 3x** (10.5s / 3.5s)</p>
            </div>
            <h2>Code Example</h2>
            <div class="syntax-block"><pre class="syntax-block"><code class="language-python">
from sigexec import Graph, GraphData
import time
import numpy as np

# Define operations with artificial delays
def expensive_load_data(delay=2.0):
    def load(_):
        time.sleep(delay)  # Simulate expensive operation
        data = np.random.randn(1000)
        return GraphData(data, metadata={&#x27;sample_rate&#x27;: 1e6})
    return load

def expensive_preprocessing(delay=1.5):
    def preprocess(signal_data):
        time.sleep(delay)  # Simulate expensive operation
        return GraphData(signal_data.data * 2, signal_data.metadata)
    return preprocess

def cheap_operation(factor):
    def process(signal_data):
        return GraphData(signal_data.data * factor, signal_data.metadata)
    return process

# With memoization (default)
start = time.time()
results_cached = (Graph(&quot;Cached&quot;, enable_cache=True)
    .add(expensive_load_data(delay=2.0))      # Runs once (2s)
    .add(expensive_preprocessing(delay=1.5))  # Runs once (1.5s)
    .variants(cheap_operation, [1.0, 2.0, 3.0], 
              names=[&#x27;1x&#x27;, &#x27;2x&#x27;, &#x27;3x&#x27;])        # Runs 3 times (fast)
    .run()
)
cached_time = time.time() - start
print(f&quot;With cache: {cached_time:.1f}s&quot;)  # ~3.5s

# Without memoization
start = time.time()
results_uncached = (Graph(&quot;Uncached&quot;, enable_cache=False)
    .add(expensive_load_data(delay=2.0))      # Runs 3 times (6s)
    .add(expensive_preprocessing(delay=1.5))  # Runs 3 times (4.5s)
    .variants(cheap_operation, [1.0, 2.0, 3.0],
              names=[&#x27;1x&#x27;, &#x27;2x&#x27;, &#x27;3x&#x27;])        # Runs 3 times (fast)
    .run()
)
uncached_time = time.time() - start
print(f&quot;Without cache: {uncached_time:.1f}s&quot;)  # ~10.5s
print(f&quot;Speedup: {uncached_time/cached_time:.1f}x&quot;)  # ~3x
</code></pre></div>
            <h2>Live Performance Comparison</h2>
            <div>
              <p>Running the same parameter exploration with and without memoization:</p>
            </div>
            <div>
              <p>\n**Running with memoization enabled...**</p>
            </div>
            <div>
              <p>\n**Running without memoization...**</p>
            </div>
            <div><table class="dataframe table-hover table-striped" id="table-0">
  <thead>
    <tr style="text-align: right;">
      <th>Configuration</th>
      <th>Execution Time</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Expected (Cached)</td>
      <td>0.3s</td>
      <td>0.2s load + 0.15s preprocess (once)</td>
    </tr>
    <tr>
      <td>Actual (Cached)</td>
      <td>1.1s</td>
      <td>Actual measured time</td>
    </tr>
    <tr>
      <td>Expected (Uncached)</td>
      <td>1.0s</td>
      <td>(0.2s + 0.15s) × 3 variants</td>
    </tr>
    <tr>
      <td>Actual (Uncached)</td>
      <td>1.1s</td>
      <td>Actual measured time</td>
    </tr>
    <tr>
      <td>Expected Speedup</td>
      <td>3.0x</td>
      <td>1.0s / 0.3s</td>
    </tr>
    <tr>
      <td>Actual Speedup</td>
      <td>1.0x</td>
      <td>Actual speedup achieved</td>
    </tr>
  </tbody>
</table></div>
            <div>
              <p>**Result**: Memoization provides a **1.0x speedup** (expected: 3.0x)!
    
    The speedup comes from:
    - Load data (0.2s) runs **1 time** instead of 3 times → saves 0.4s
    - Preprocess (0.15s) runs **1 time** instead of 3 times → saves 0.3s
    - Scaling operations run 3 times in both cases (can't be shared)
    
    **Total time saved: -0.0 seconds** (-0% reduction)</p>
            </div>
            <h2>Scaling with Parameter Space Size</h2>
            <div>
              <p>The benefits of memoization scale linearly with the number of parameter variants.
    Let's see how speedup changes with different numbers of variants:</p>
            </div>
            <div><table class="dataframe table-hover table-striped" id="table-0">
  <thead>
    <tr style="text-align: right;">
      <th>Variants</th>
      <th>Time (Cached)</th>
      <th>Time (Uncached)</th>
      <th>Expected Speedup</th>
      <th>Actual Speedup</th>
      <th>Time Saved</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2</td>
      <td>2.00s</td>
      <td>2.00s</td>
      <td>2.0x</td>
      <td>1.0x</td>
      <td>-0.00s</td>
    </tr>
    <tr>
      <td>3</td>
      <td>3.00s</td>
      <td>3.00s</td>
      <td>3.0x</td>
      <td>1.0x</td>
      <td>0.00s</td>
    </tr>
    <tr>
      <td>4</td>
      <td>4.01s</td>
      <td>4.01s</td>
      <td>4.0x</td>
      <td>1.0x</td>
      <td>-0.00s</td>
    </tr>
    <tr>
      <td>5</td>
      <td>5.01s</td>
      <td>5.01s</td>
      <td>5.0x</td>
      <td>1.0x</td>
      <td>0.00s</td>
    </tr>
  </tbody>
</table></div>
            <div>
              <p>As you can see, the speedup scales linearly with the number of variants:
    - 2 variants → ~2x speedup
    - 3 variants → ~3x speedup
    - 4 variants → ~4x speedup
    - 5 variants → ~5x speedup
    
    This is because:
    - **Cached version**: Expensive operations run once regardless of variant count
    - **Uncached version**: Expensive operations run N times for N variants
    - **Speedup = N** (number of variants)</p>
            </div>
            <h2>Execution Details</h2>
            <div>
              <p>Here's what happens during execution:</p>
            </div>
            <div><table class="dataframe table-hover table-striped" id="table-0">
  <thead>
    <tr style="text-align: right;">
      <th>Stage</th>
      <th>With Cache</th>
      <th>Without Cache</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Load Data</td>
      <td>0.2s (executed)</td>
      <td>0.2s × 3 = 0.6s</td>
    </tr>
    <tr>
      <td>Preprocess</td>
      <td>0.15s (executed)</td>
      <td>0.15s × 3 = 0.4s</td>
    </tr>
    <tr>
      <td>Scale 1x</td>
      <td><0.1s (executed)</td>
      <td><0.1s (executed)</td>
    </tr>
    <tr>
      <td>Scale 2x</td>
      <td><0.1s (executed)</td>
      <td><0.1s (executed)</td>
    </tr>
    <tr>
      <td>Scale 3x</td>
      <td><0.1s (executed)</td>
      <td><0.1s (executed)</td>
    </tr>
    <tr>
      <td>TOTAL</td>
      <td>~1.0s</td>
      <td>~5.0s</td>
    </tr>
  </tbody>
</table></div>
            <div>
              <p>The cached version skips re-execution of expensive stages by reusing stored results.
    Each variant (Scale 1x, 2x, 3x) gets the same preprocessed data without waiting!</p>
            </div>
            <h2>Best Practices</h2>
            <div>
              <p>To maximize the benefits of memoization:
    
    1. **Structure pipelines strategically**: Put expensive operations that are common 
       across variants early in the graph, before the first `.variants()` call.
    
    2. **Use `.variants()` not loops**: Instead of manually looping over parameters,
       use `.variants()` to let the framework handle caching automatically.
    
    3. **Group similar explorations**: If exploring multiple parameter dimensions,
       do them in one graph rather than separate runs.
    
    4. **Clear cache when needed**: The cache is shared across all Graph instances.
       Clear it with `Graph._global_cache.clear()` when starting a new experiment.
    
    5. **Disable cache for debugging**: Use `enable_cache=False` when debugging or when
       you explicitly want fresh execution (e.g., testing randomness).</p>
            </div>
            <h2>When Memoization Helps Most</h2>
            <div>
              <p>Memoization provides the biggest benefits when:
    
    - **Exploring parameter spaces**: Testing different processing parameters
    - **Expensive early stages**: Signal generation, filtering, or other costly operations
    - **Many combinations**: Large cartesian products of variants
    - **Branching pipelines**: Multiple processing paths from the same source
    - **Iterative development**: Re-running similar experiments during development
    
    It's less helpful when:
    - Running a single linear graph (no variants)
    - Every stage is unique (no shared computation)
    - Intermediate results are very large (cache memory overhead)</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>